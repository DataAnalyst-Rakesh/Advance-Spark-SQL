{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF+lhtrmwpZRrw7A6CN+Gi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataAnalyst-Rakesh/Advance-Spark-SQL/blob/main/Advanced_Spark_SQL_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced SQL Operations with Spark SQL\n",
        "\n",
        "# Overview\n",
        "\n",
        "This project demonstrates the application of Spark SQL for data manipulation, querying, and advanced analytics. It uses well-structured datasets to showcase the versatility of SQL in handling real-world data.**bold text**"
      ],
      "metadata": {
        "id": "a8KTguge4p_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import sys\n",
        "import os\n",
        "python_path = sys.executable\n",
        "os.environ['PYSPARK_PYTHON'] = python_path\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "conf = SparkConf().setAppName(\"pyspark\").setMaster(\"local[*]\").set(\"spark.driver.host\",\"localhost\").set(\"spark.driver.allowMultipleContexts\",\"true\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.config(\"spark.driver.allowMultipleContexts\",\"true\").getOrCreate()\n",
        "print(\"DONE==== START YOUR WORKðŸ‘‡ \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeB8qJr244Fr",
        "outputId": "c43da8b6-9e3d-483c-db45-07f609136f3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE==== START YOUR WORKðŸ‘‡ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Data Preparation\n",
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Advanced SQL Operations\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create the main dataset\n",
        "data = [\n",
        "    (0, '06-26-2011', 300.4, 'Exercise', 'GymnasticsPro', 'cash'),\n",
        "    (1, '05-26-2011', 200.0, 'Exercise Band', 'Weightlifting', 'credit'),\n",
        "    (2, '06-01-2011', 300.4, 'Exercise', 'Gymnastics Pro', 'cash'),\n",
        "    (3, '06-05-2011', 100.0, 'Gymnastics', 'Rings', 'credit'),\n",
        "    (4, '12-17-2011', 300.0, 'Team Sports', 'Field', 'cash'),\n",
        "    (5, '02-14-2011', 200.0, 'Gymnastics', None, 'cash'),\n",
        "    (6, '06-05-2011', 100.0, 'Exercise', 'Rings', 'credit'),\n",
        "    (7, '12-17-2011', 300.0, 'Team Sports', 'Field', 'cash'),\n",
        "    (8, '02-14-2011', 200.0, 'Gymnastics', None, 'cash'),\n",
        "]\n",
        "\n",
        "columns = ['id', 'tdate', 'amount', 'category', 'product', 'spendby']\n",
        "\n",
        "# Load dataset into a Spark DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Register the DataFrame as a SQL view for querying\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "\n",
        "# Display the dataset\n",
        "print(\"Dataset Preview:\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDdmyC_v5AyF",
        "outputId": "57635602-b696-4470-b0e6-ff186689db54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Basic SQL Queries\n",
        "# Display all data\n",
        "spark.sql(\"SELECT * FROM df\").show()\n",
        "\n",
        "# Count the number of rows\n",
        "spark.sql(\"SELECT COUNT(*) AS row_count FROM df\").show()\n",
        "\n",
        "# Find the maximum and minimum ID\n",
        "spark.sql(\"SELECT MAX(id) AS max_id, MIN(id) AS min_id FROM df\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6DYykzO5MQJ",
        "outputId": "6df8176e-08e8-4aeb-dd4f-314b9d9b6dcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n",
            "+---------+\n",
            "|row_count|\n",
            "+---------+\n",
            "|        9|\n",
            "+---------+\n",
            "\n",
            "+------+------+\n",
            "|max_id|min_id|\n",
            "+------+------+\n",
            "|     8|     0|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Filtering and Conditions\n",
        "# Filter rows where category is 'Exercise'\n",
        "spark.sql(\"SELECT * FROM df WHERE category = 'Exercise'\").show()\n",
        "\n",
        "# Filter rows with multiple categories\n",
        "spark.sql(\"SELECT * FROM df WHERE category IN ('Exercise', 'Gymnastics')\").show()\n",
        "\n",
        "# Filter rows where product column is NULL or NOT NULL\n",
        "spark.sql(\"SELECT * FROM df WHERE product IS NULL\").show()\n",
        "spark.sql(\"SELECT * FROM df WHERE product IS NOT NULL\").show()\n",
        "\n",
        "# Add a new column for cash/credit\n",
        "spark.sql(\"\"\"\n",
        "SELECT *,\n",
        "    CASE WHEN spendby = 'cash' THEN 1 ELSE 0 END AS MOP\n",
        "FROM df\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnvTiXKr6ClC",
        "outputId": "d13fc601-4d8d-45b2-dca2-b4af9d88a433"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+--------+--------------+-------+\n",
            "| id|     tdate|amount|category|       product|spendby|\n",
            "+---+----------+------+--------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|Exercise| GymnasticsPro|   cash|\n",
            "|  2|06-01-2011| 300.4|Exercise|Gymnastics Pro|   cash|\n",
            "|  6|06-05-2011| 100.0|Exercise|         Rings| credit|\n",
            "+---+----------+------+--------+--------------+-------+\n",
            "\n",
            "+---+----------+------+----------+--------------+-------+\n",
            "| id|     tdate|amount|  category|       product|spendby|\n",
            "+---+----------+------+----------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|  Exercise| GymnasticsPro|   cash|\n",
            "|  2|06-01-2011| 300.4|  Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|Gymnastics|         Rings| credit|\n",
            "|  5|02-14-2011| 200.0|Gymnastics|          null|   cash|\n",
            "|  6|06-05-2011| 100.0|  Exercise|         Rings| credit|\n",
            "|  8|02-14-2011| 200.0|Gymnastics|          null|   cash|\n",
            "+---+----------+------+----------+--------------+-------+\n",
            "\n",
            "+---+----------+------+----------+-------+-------+\n",
            "| id|     tdate|amount|  category|product|spendby|\n",
            "+---+----------+------+----------+-------+-------+\n",
            "|  5|02-14-2011| 200.0|Gymnastics|   null|   cash|\n",
            "|  8|02-14-2011| 200.0|Gymnastics|   null|   cash|\n",
            "+---+----------+------+----------+-------+-------+\n",
            "\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n",
            "+---+----------+------+-------------+--------------+-------+---+\n",
            "| id|     tdate|amount|     category|       product|spendby|MOP|\n",
            "+---+----------+------+-------------+--------------+-------+---+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|  1|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|  0|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|  1|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|  0|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|  1|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          null|   cash|  1|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|  0|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|  1|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          null|   cash|  1|\n",
            "+---+----------+------+-------------+--------------+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Aggregations\n",
        "# Sum, max, and min amounts by category\n",
        "spark.sql(\"\"\"\n",
        "SELECT category,\n",
        "       SUM(amount) AS total_amount,\n",
        "       MAX(amount) AS max_amount,\n",
        "       MIN(amount) AS min_amount\n",
        "FROM df\n",
        "GROUP BY category\n",
        "\"\"\").show()\n",
        "\n",
        "# Group by multiple dimensions\n",
        "spark.sql(\"\"\"\n",
        "SELECT category, spendby,\n",
        "       SUM(amount) AS total_amount,\n",
        "       COUNT(*) AS transaction_count\n",
        "FROM df\n",
        "GROUP BY category, spendby\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxK8K3pR6Los",
        "outputId": "3a7b95e1-0f77-4d19-837f-46c918403afc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+----------+----------+\n",
            "|     category|total_amount|max_amount|min_amount|\n",
            "+-------------+------------+----------+----------+\n",
            "|   Gymnastics|       500.0|     200.0|     100.0|\n",
            "|  Team Sports|       600.0|     300.0|     300.0|\n",
            "|     Exercise|       700.8|     300.4|     100.0|\n",
            "|Exercise Band|       200.0|     200.0|     200.0|\n",
            "+-------------+------------+----------+----------+\n",
            "\n",
            "+-------------+-------+------------+-----------------+\n",
            "|     category|spendby|total_amount|transaction_count|\n",
            "+-------------+-------+------------+-----------------+\n",
            "|   Gymnastics|   cash|       400.0|                2|\n",
            "|     Exercise|   cash|       600.8|                2|\n",
            "|     Exercise| credit|       100.0|                1|\n",
            "|  Team Sports|   cash|       600.0|                2|\n",
            "|Exercise Band| credit|       200.0|                1|\n",
            "|   Gymnastics| credit|       100.0|                1|\n",
            "+-------------+-------+------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.String Operations\n",
        "# Concatenate id and category\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, category,\n",
        "       CONCAT(id, '-', category) AS condata\n",
        "FROM df\n",
        "\"\"\").show()\n",
        "\n",
        "# Convert category to lowercase and uppercase\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, category,\n",
        "       LOWER(category) AS lowcat,\n",
        "       UPPER(category) AS upcat\n",
        "FROM df\n",
        "\"\"\").show()\n",
        "\n",
        "# Trim spaces from the product column\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, product,\n",
        "       TRIM(product) AS trimmed_product\n",
        "FROM df\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ro952Ix6n5G",
        "outputId": "09556cf7-6869-4574-a7ba-79c565b73af6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+---------------+\n",
            "| id|     category|        condata|\n",
            "+---+-------------+---------------+\n",
            "|  0|     Exercise|     0-Exercise|\n",
            "|  1|Exercise Band|1-Exercise Band|\n",
            "|  2|     Exercise|     2-Exercise|\n",
            "|  3|   Gymnastics|   3-Gymnastics|\n",
            "|  4|  Team Sports|  4-Team Sports|\n",
            "|  5|   Gymnastics|   5-Gymnastics|\n",
            "|  6|     Exercise|     6-Exercise|\n",
            "|  7|  Team Sports|  7-Team Sports|\n",
            "|  8|   Gymnastics|   8-Gymnastics|\n",
            "+---+-------------+---------------+\n",
            "\n",
            "+---+-------------+-------------+-------------+\n",
            "| id|     category|       lowcat|        upcat|\n",
            "+---+-------------+-------------+-------------+\n",
            "|  0|     Exercise|     exercise|     EXERCISE|\n",
            "|  1|Exercise Band|exercise band|EXERCISE BAND|\n",
            "|  2|     Exercise|     exercise|     EXERCISE|\n",
            "|  3|   Gymnastics|   gymnastics|   GYMNASTICS|\n",
            "|  4|  Team Sports|  team sports|  TEAM SPORTS|\n",
            "|  5|   Gymnastics|   gymnastics|   GYMNASTICS|\n",
            "|  6|     Exercise|     exercise|     EXERCISE|\n",
            "|  7|  Team Sports|  team sports|  TEAM SPORTS|\n",
            "|  8|   Gymnastics|   gymnastics|   GYMNASTICS|\n",
            "+---+-------------+-------------+-------------+\n",
            "\n",
            "+---+--------------+---------------+\n",
            "| id|       product|trimmed_product|\n",
            "+---+--------------+---------------+\n",
            "|  0| GymnasticsPro|  GymnasticsPro|\n",
            "|  1| Weightlifting|  Weightlifting|\n",
            "|  2|Gymnastics Pro| Gymnastics Pro|\n",
            "|  3|         Rings|          Rings|\n",
            "|  4|         Field|          Field|\n",
            "|  5|          null|           null|\n",
            "|  6|         Rings|          Rings|\n",
            "|  7|         Field|          Field|\n",
            "|  8|          null|           null|\n",
            "+---+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Window Functions\n",
        "# Add a row number partitioned by category\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, category, amount,\n",
        "       ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) AS row_num\n",
        "FROM df\n",
        "\"\"\").show()\n",
        "\n",
        "# Add rank and dense rank\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, category, amount,\n",
        "       RANK() OVER (PARTITION BY category ORDER BY amount DESC) AS rank_num,\n",
        "       DENSE_RANK() OVER (PARTITION BY category ORDER BY amount DESC) AS dense_rank_num\n",
        "FROM df\n",
        "\"\"\").show()\n",
        "\n",
        "# Lead and lag\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, category, amount,\n",
        "       LEAD(amount) OVER (PARTITION BY category ORDER BY amount DESC) AS lead_amount,\n",
        "       LAG(amount) OVER (PARTITION BY category ORDER BY amount DESC) AS lag_amount\n",
        "FROM df\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brZCTDGf651h",
        "outputId": "1826bb4d-ce2d-463a-e1b0-73b4223662bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------+-------+\n",
            "| id|     category|amount|row_num|\n",
            "+---+-------------+------+-------+\n",
            "|  5|   Gymnastics| 200.0|      1|\n",
            "|  8|   Gymnastics| 200.0|      2|\n",
            "|  3|   Gymnastics| 100.0|      3|\n",
            "|  4|  Team Sports| 300.0|      1|\n",
            "|  7|  Team Sports| 300.0|      2|\n",
            "|  0|     Exercise| 300.4|      1|\n",
            "|  2|     Exercise| 300.4|      2|\n",
            "|  6|     Exercise| 100.0|      3|\n",
            "|  1|Exercise Band| 200.0|      1|\n",
            "+---+-------------+------+-------+\n",
            "\n",
            "+---+-------------+------+--------+--------------+\n",
            "| id|     category|amount|rank_num|dense_rank_num|\n",
            "+---+-------------+------+--------+--------------+\n",
            "|  5|   Gymnastics| 200.0|       1|             1|\n",
            "|  8|   Gymnastics| 200.0|       1|             1|\n",
            "|  3|   Gymnastics| 100.0|       3|             2|\n",
            "|  4|  Team Sports| 300.0|       1|             1|\n",
            "|  7|  Team Sports| 300.0|       1|             1|\n",
            "|  0|     Exercise| 300.4|       1|             1|\n",
            "|  2|     Exercise| 300.4|       1|             1|\n",
            "|  6|     Exercise| 100.0|       3|             2|\n",
            "|  1|Exercise Band| 200.0|       1|             1|\n",
            "+---+-------------+------+--------+--------------+\n",
            "\n",
            "+---+-------------+------+-----------+----------+\n",
            "| id|     category|amount|lead_amount|lag_amount|\n",
            "+---+-------------+------+-----------+----------+\n",
            "|  5|   Gymnastics| 200.0|      200.0|      null|\n",
            "|  8|   Gymnastics| 200.0|      100.0|     200.0|\n",
            "|  3|   Gymnastics| 100.0|       null|     200.0|\n",
            "|  4|  Team Sports| 300.0|      300.0|      null|\n",
            "|  7|  Team Sports| 300.0|       null|     300.0|\n",
            "|  0|     Exercise| 300.4|      300.4|      null|\n",
            "|  2|     Exercise| 300.4|      100.0|     300.4|\n",
            "|  6|     Exercise| 100.0|       null|     300.4|\n",
            "|  1|Exercise Band| 200.0|       null|      null|\n",
            "+---+-------------+------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Date Operations\n",
        "# Convert and format dates\n",
        "spark.sql(\"\"\"\n",
        "SELECT id, tdate,\n",
        "       DATE_FORMAT(tdate, 'yyyy-MM-dd') AS formatted_date\n",
        "FROM df\n",
        "\"\"\").show()\n",
        "\n",
        "# Subquery to filter rows where the amount is above the average\n",
        "spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM df\n",
        "WHERE amount > (SELECT AVG(amount) FROM df)\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umHKvSuc7IsR",
        "outputId": "a4a77bd0-e27d-4b3d-a9b3-fa30e4a5f119"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------+\n",
            "| id|     tdate|formatted_date|\n",
            "+---+----------+--------------+\n",
            "|  0|06-26-2011|          null|\n",
            "|  1|05-26-2011|          null|\n",
            "|  2|06-01-2011|          null|\n",
            "|  3|06-05-2011|          null|\n",
            "|  4|12-17-2011|          null|\n",
            "|  5|02-14-2011|          null|\n",
            "|  6|06-05-2011|          null|\n",
            "|  7|12-17-2011|          null|\n",
            "|  8|02-14-2011|          null|\n",
            "+---+----------+--------------+\n",
            "\n",
            "+---+----------+------+-----------+--------------+-------+\n",
            "| id|     tdate|amount|   category|       product|spendby|\n",
            "+---+----------+------+-----------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|   Exercise| GymnasticsPro|   cash|\n",
            "|  2|06-01-2011| 300.4|   Exercise|Gymnastics Pro|   cash|\n",
            "|  4|12-17-2011| 300.0|Team Sports|         Field|   cash|\n",
            "|  7|12-17-2011| 300.0|Team Sports|         Field|   cash|\n",
            "+---+----------+------+-----------+--------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Adding Joins\n",
        "# Create additional datasets for join examples\n",
        "cust_data = [(1, 'raj'), (2, 'ravi'), (3, 'sai'), (5, 'rani')]\n",
        "prod_data = [(1, 'mouse'), (3, 'mobile'), (7, 'laptop')]\n",
        "\n",
        "# Create DataFrames for customers and products\n",
        "cust = spark.createDataFrame(cust_data, ['id', 'name'])\n",
        "prod = spark.createDataFrame(prod_data, ['id', 'product'])\n",
        "\n",
        "# Register these DataFrames as SQL views\n",
        "cust.createOrReplaceTempView(\"cust\")\n",
        "prod.createOrReplaceTempView(\"prod\")\n",
        "\n",
        "# Preview the datasets\n",
        "print(\"Customer Data:\")\n",
        "cust.show()\n",
        "\n",
        "print(\"Product Data:\")\n",
        "prod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yqi7G9-7jyK",
        "outputId": "abcbc9c5-9777-4c78-e855-482d02c495bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Data:\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "Product Data:\n",
            "+---+-------+\n",
            "| id|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Inner Join\n",
        "# Perform an inner join between customers and products\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name, prod.product\n",
        "FROM cust\n",
        "INNER JOIN prod\n",
        "ON cust.id = prod.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py7CDL6a7sPT",
        "outputId": "d781cb97-8299-48e9-e300-e69e1cd8d9b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Left Join\n",
        "# Perform a left join to include all customers\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name, prod.product\n",
        "FROM cust\n",
        "LEFT JOIN prod\n",
        "ON cust.id = prod.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3OGXsHd7wad",
        "outputId": "dd8dc883-5e0a-442a-bbfb-37876fcb5bd8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  5|rani|   null|\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "|  2|ravi|   null|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Right Join\n",
        "# Perform a right join to include all products\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name, prod.product\n",
        "FROM cust\n",
        "RIGHT JOIN prod\n",
        "ON cust.id = prod.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06OFw-MQ71qh",
        "outputId": "f035db80-e88f-47d2-d164-c2684a53420c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|  id|name|product|\n",
            "+----+----+-------+\n",
            "|null|null| laptop|\n",
            "|   1| raj|  mouse|\n",
            "|   3| sai| mobile|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Full Outer Join\n",
        "# Perform a full outer join to include all customers and products\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name, prod.product\n",
        "FROM cust\n",
        "FULL OUTER JOIN prod\n",
        "ON cust.id = prod.id\n",
        "ORDER BY cust.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBhvzdtN72Mx",
        "outputId": "9921e9a5-7a2f-42a6-ea88-b779bb61806c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|  id|name|product|\n",
            "+----+----+-------+\n",
            "|null|null| laptop|\n",
            "|   1| raj|  mouse|\n",
            "|   2|ravi|   null|\n",
            "|   3| sai| mobile|\n",
            "|   5|rani|   null|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Anti Join\n",
        "# Perform an anti join to find customers without a matching product\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name\n",
        "FROM cust\n",
        "LEFT ANTI JOIN prod\n",
        "ON cust.id = prod.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zpAciKq8G39",
        "outputId": "5dd6d4e6-604a-493a-df3d-5d1b4e76b03e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  5|rani|\n",
            "|  2|ravi|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Semi Join\n",
        "# Perform a semi join to find customers with a matching product\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id, cust.name\n",
        "FROM cust\n",
        "LEFT SEMI JOIN prod\n",
        "ON cust.id = prod.id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzwiJhxG8MLG",
        "outputId": "bbc80a49-2f40-46d8-c626-95a6d8e0425f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  3| sai|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Cross Join\n",
        "# Perform a cross join between customers and products\n",
        "spark.sql(\"\"\"\n",
        "SELECT cust.id AS cust_id, cust.name AS customer_name,\n",
        "       prod.id AS prod_id, prod.product AS product_name\n",
        "FROM cust\n",
        "CROSS JOIN prod\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0-sS5oD8M5Z",
        "outputId": "ccec15cc-5d63-43ff-c691-376a874ad07f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-------+------------+\n",
            "|cust_id|customer_name|prod_id|product_name|\n",
            "+-------+-------------+-------+------------+\n",
            "|      1|          raj|      1|       mouse|\n",
            "|      2|         ravi|      1|       mouse|\n",
            "|      1|          raj|      3|      mobile|\n",
            "|      1|          raj|      7|      laptop|\n",
            "|      2|         ravi|      3|      mobile|\n",
            "|      2|         ravi|      7|      laptop|\n",
            "|      3|          sai|      1|       mouse|\n",
            "|      5|         rani|      1|       mouse|\n",
            "|      3|          sai|      3|      mobile|\n",
            "|      3|          sai|      7|      laptop|\n",
            "|      5|         rani|      3|      mobile|\n",
            "|      5|         rani|      7|      laptop|\n",
            "+-------+-------------+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joins Summary\n",
        "\n",
        "Join Type \tDescription  <br>\n",
        "1) Inner Join -\tIncludes rows with matching keys in both datasets. <br>\n",
        "2) Left Join\t- Includes all rows from the left dataset and matching rows from the right. <br>\n",
        "3) Right Join - Includes all rows from the right dataset and matching rows from the left.  <br>\n",
        "4) Full Join - Includes all rows from both datasets, matching where possible.  <br>\n",
        "5) Anti Join - Includes rows from the left dataset that have no match in the right.  <br>\n",
        "6) Semi Join - Includes rows from the left dataset that have a match in the right. <br>\n",
        "7) Cross Join - Includes every combination of rows from both datasets (Cartesian Product)."
      ],
      "metadata": {
        "id": "KLqyMA5H9FxN"
      }
    }
  ]
}